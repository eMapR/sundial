model:
    class_path: PrithviFCN
    init_args:
        num_classes: 1
        view_size: 16
        upscale_depth: 2
        prithvi_path: src/backbones/prithvi/Prithvi_100M.pt
        prithvi_params:
            model_args:
                decoder_depth: 8
                decoder_embed_dim: 512
                decoder_num_heads: 16
                depth: 12
                embed_dim: 768
                img_size: 256
                in_chans: 6
                num_frames: 3
                num_heads: 12
                patch_size: 16
                tubelet_size: 1
            train_params:
                mask_ratio: 0.0
        prithvi_freeze: true
        criterion: "bce"
data:
    class_path: ChipsDataModule
    init_args:
        batch_size: 16
        num_workers: 64
        back_step: null
        base_year: null
        chip_size: 256
        means:
            - 775.2290211032589
            - 1080.992780391705
            - 1228.5855250417867
            - 2497.2022620507532
            - 2204.2139147975554
            - 1610.8324823273745
        stds:
            - 1281.526139861424
            - 1270.0297974547493
            - 1399.4802505642526
            - 1368.3446143747644
            - 1291.6764008585435
            - 1154.505683480695
trainer:
    callbacks:
      - class_path: PrithviFCNCallbacks
    log_every_n_steps: 16
    max_epochs: 256
    strategy: ddp_find_unused_parameters_true
    devices: 2
optimizer:
    class_path: torch.optim.AdamW
    init_args:
        lr: 0.0005
lr_scheduler:
    class_path: torch.optim.lr_scheduler.StepLR
    init_args:
        step_size: 64
        gamma: 0.5

